---
title: "Denoising Autoencoder for Bioacoustic Applications"
excerpt: "Class project for CS281: Advanced Machine Learning, Fall 2019. <br/><img src='/images/500x300.png' style='height:300px;'>"
collection: portfolio
---

*This work was part of a final project for Harvard's graduate class CS281: "Advanced Machine Learning", fall semester of 2019, done together with Petur Bryde, Serges Saidi, and Tyler Piazza. The work described are the components I was primarily responsible for.*  

While it is well known that wildlife populations have been decreasing over the past decades in response to increased many anthropogenic factors, such as habitat loss, pollution, and global warming -- but the rate of decline can be hard to quantify. To this end, a recent study conducted a large-scale review of individual surveys, and estimated a 30% loss in North American bird populations since the 1970s — amounting to about 3 billion birds \cite{Rosenberg120}. While it is crucial to inact effective conservation strategies, their evaluation and success depends on accurate populations estimates of different species. Unfortunately, this is traditionally a very tedious and time-intensive task, requiring ecologists to manually surveys plots of an environment, record visual observations, and statistically extrapolate their findings to a larger area. However, there has been a shift towards automated monitoring systems in recent decades — most notably in acoustic sensors, which can be deployed throughout an environment, record the natural soundscape, and can later be collected and analyzed. Such sensors can target any vocal taxa, ranging from frogs, to bats, to elephants — but are particularly relevant for birds, which have very prominent, and well-studied vocal signatures.

On the whole, the use of acoustic sensors could greatly simplify population surveys, saving time and resources. Yet they do introduce a new technological challenge: how can birdsong be identified and classified within many hours worth of audio recordings?  Indeed, this is an (see \cite{Stowell2016, priyadarshani2018} for reviews) and has been the subject of several public data challenges \cite{Stowell2018, BirdCLEF2019}. One of the biggest barriers is the considerable background noise present in most field recordings, including wind, rain, traffic, crickets, and other nearby fauna. Since such noise is highly time-variant and region-dependent, many systems struggle to generalize to data collected across different environmental conditions. Our goal is to mitigate this issue by drawing on work from deep-learning-based audio and image enhancement to isolate birdsong from background noise.

First, we construct multiple datasets on which to train our system. A major difficulty of implementing acoustic foreground-background segmentation on natural data is that the ground-truth will generally not be available. In particular, for our application, we do not expect it to be possible to collect recordings of birds in the wild without simultaneously capturing the surrounding soundscape. On the other hand, given a foreground and background, it is very easy to construct the overall mixed signal. Therefore, by collecting separate recordings representing candidate background noise and candidate birdsong, we can artifically construct viable mixed recordings.

Next, we will implement and test a convolutional de-noising autoencoder (CDAE).

![](/images/spectrogram_realistic.png)
